{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Clear everything for a fresh file \"\"\"\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Delete files in 'support' folder if it exists\n",
    "if os.path.exists('support'):\n",
    "    shutil.rmtree('support')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Initial Query with ChatGPT\n",
    "\n",
    "Here, we initialize our product and question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dear user, enter your Product and Question here!\n",
    "\"\"\"\n",
    "\n",
    "product = \"PICO 4 All-in-One VR Headset\"\n",
    "question =  f\"Identify and rank ten opportunities for design improvements with the {product}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Set up OpenAI API key \"\"\"\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\"\"\" Use ChatGPT \"\"\"\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "template = \" Express the answer only as a Python dictionary with the key as a physical component and the value as a concised explanation with a maximum of one sentence. If you don't know the answer to the question, strictly state 'I don't know'.\"\n",
    "prompt = question + template\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt,}],\n",
    "    temperature=0.5,                                        # adjust persona and temperature\n",
    ")\n",
    "\n",
    "result = chat_completion.choices[0].message.content\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: Data Collection and Preprocessing\n",
    "\n",
    "We will be scrapping data from: \\\n",
    "**Social Media** \\\n",
    "[1] Youtube Comments \\\n",
    "[2] Reddit Comments \\\n",
    "**Websites** \\\n",
    "[3] Official Product Website \\\n",
    "[4] Tech Magazine PCGamer \\\n",
    "**PDF file** \\\n",
    "[5] Official User Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2.1: Youtube Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Initialise and Set up Google API Key \"\"\"\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "youtube = build('youtube', 'v3', developerKey=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dear user, adjust the number of results to ensure a good sample size of around 3000 comments. \n",
    "\"\"\"\n",
    "search_terms = product\n",
    "max_result = 20             # No. of results (1-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define containers to store info \"\"\"\n",
    "vid_id = []             \t# video id\n",
    "vid_page = []       \t\t# video links (https...)\n",
    "vid_title = []              # video title\n",
    "num_comments = []           # official number of comments\n",
    "load_error = 0              # error counter\n",
    "can_load_title = []         # temp. list for storing title w/o loading error\n",
    "can_load_page = []          # temp. list for storing links w/o loading error\n",
    "num_page = []               # comment_response page number\n",
    "page_title = []             # comment_response video title\n",
    "comment_resp = []           # comment_response\n",
    "comment_list = []           # temp. list for storing comments\n",
    "comment_data = []           # comments & replies from comment_response\n",
    "all_count = 0               # total number of comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Search for Video IDs based on User Inputs \"\"\"\n",
    "print(\"Search for Videos IDs...\")\n",
    "request = youtube.search().list(\n",
    "    q=search_terms,\n",
    "    maxResults=max_result,\n",
    "    part=\"id\",\n",
    "    type=\"video\",\n",
    "    order=\"relevance\"         # Switch to \"viewCount\" if the number of comments are not sufficient\n",
    "    )\n",
    "search_response = request.execute()\n",
    "print(search_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create a list of Video IDs and a corresponding list of weblinks \"\"\"\n",
    "print(\"Videos found...\")\n",
    "for i in range(max_result):\n",
    "    videoId = search_response['items'][i]['id']['videoId']\n",
    "    print(videoId)\n",
    "    vid_id.append(videoId)                          # a list of Video IDs\n",
    "    page = \"https://www.youtube.com/watch?v=\" + videoId\n",
    "    print(page)\n",
    "    print()\n",
    "    vid_page.append(page)                           # a list of Video links\n",
    "print(\"\\nThere are\", len(vid_page), \"videos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Use the list of Video IDs to get video data \"\"\"\n",
    "print(\"Get video data...\")\n",
    "for i in range(len(vid_id)):\n",
    "    request = youtube.videos().list(\n",
    "        part=\"snippet, statistics\",\n",
    "        id=vid_id[i]\n",
    "        )\n",
    "    video_response = request.execute()\n",
    "    print(video_response)\n",
    "\n",
    "    title = video_response['items'][0]['snippet']['title']\n",
    "    vid_title.append(title)\n",
    "    try:                        # use try/except as some videos might not load\n",
    "        comment_count = video_response['items'][0]['statistics']['commentCount']\n",
    "        print(\"Video\", i + 1, \"-\", title, \"-- Comment count: \", comment_count)\n",
    "        print()\n",
    "        num_comments.append(comment_count)\n",
    "    except:\n",
    "        print(\"Video\", i + 1, \"-\", title, \"-- Comments are turned off\")\n",
    "        print()\n",
    "        num_comments.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Use the list of Video IDs to get comments (by page) \"\"\"\n",
    "print(\"Get comment data...\")\n",
    "for i in range(len(vid_id)):\n",
    "    try:                                        # use try/except as some \"comments are turned off\"\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet,replies\",\n",
    "            videoId=vid_id[i]\n",
    "            )\n",
    "        comment_response = request.execute()\n",
    "        print(comment_response)\n",
    "\n",
    "        comment_resp.append(comment_response)   # append 1 page of comment_response\n",
    "        pages = 1\n",
    "        num_page.append(pages)                  # append page number of comment_response\n",
    "        page_title.append(vid_title[i])         # append video title along with the comment_response\n",
    "\n",
    "        can_load_page.append(vid_page[i])       # drop link if it can't load (have at least 1 comment page)\n",
    "        can_load_title.append(vid_title[i])     # drop title if it can't load (have at least 1 comment page)\n",
    "\n",
    "        test = comment_response.get('nextPageToken', 'nil')         # check for nextPageToken\n",
    "        while test != 'nil':                                        # keep running until last comment page\n",
    "            next_page_ = comment_response.get('nextPageToken')\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet,replies\",\n",
    "                pageToken=next_page_,\n",
    "                videoId=vid_id[i]\n",
    "                )\n",
    "            comment_response = request.execute()\n",
    "            print(comment_response)\n",
    "\n",
    "            comment_resp.append(comment_response)                   # append next page of comment_response\n",
    "            pages += 1\n",
    "            num_page.append(pages)                                  # append page number of comment_response\n",
    "            page_title.append(vid_title[i])                         # append video title along with the comment_response\n",
    "\n",
    "            test = comment_response.get('nextPageToken', 'nil')     # check for nextPageToken (while loop)\n",
    "    except:\n",
    "        load_error += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Show videos without loading errors \"\"\"\n",
    "print(\"Videos that can load...\")\n",
    "vid_page = can_load_page                    # update vid_page with those with no load error\n",
    "vid_title = can_load_title                  # update vid_title with those with no load error\n",
    "for i in range(len(vid_title)):\n",
    "    if vid_title[i] == 'YouTube':           # default error title is 'YouTube'\n",
    "        vid_title[i] = 'Video_' + str(i+1)  # replace 'YouTube' with Video_1 format\n",
    "    print(i + 1, vid_title[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Sift through and store comments as a list \"\"\"\n",
    "print(\"Get individual comment...\")\n",
    "for k in range(len(comment_resp)):\n",
    "    count = 0                                                     # comment counter\n",
    "    comments_found = comment_resp[k]['pageInfo']['totalResults']  # comments on 1 comment_response page\n",
    "    count = count + comments_found\n",
    "    for i in range(comments_found):\n",
    "        try:\n",
    "            comment_list.append(comment_resp[k]['items'][i]['snippet']['topLevelComment']['snippet']['textDisplay'])\n",
    "            print(comment_resp[k]['items'][i]['snippet']['topLevelComment']['snippet']['textDisplay'])\n",
    "        except:\n",
    "            print(\"missing comment\")                              # or too many comments (e.g. 7.3K comments)\n",
    "\n",
    "print(comment_list)\n",
    "print()\n",
    "print(len(comment_list), \"comments in total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create directory \"\"\"\n",
    "try:                                              # Create directory named after search terms\n",
    "    os.makedirs(\"support/%s\" % search_terms)\n",
    "    print(\"Directory\", search_terms, \"created\")\n",
    "except FileExistsError:\n",
    "    print(\"Directory\", search_terms, \"exists\")\n",
    "\n",
    "try:                                              # Create directory to store current search terms\n",
    "    os.makedirs(\"support/_current_\")\n",
    "    print(\"Directory _current_ created\")\n",
    "except FileExistsError:\n",
    "    print(\"Directory _current_ exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save files for future use \"\"\"\n",
    "import pickle\n",
    "\n",
    "f = open(\"support/%s/comments.txt\" % search_terms, \"w+\", encoding=\"utf-8\")\n",
    "for i in range(len(comment_list)):\n",
    "    f.write(\"<<<\" + comment_list[i] + \">>>\")\n",
    "f.close()\n",
    "\n",
    "pickle.dump(search_terms, open(\"support/%s/searchTerms.pkl\" % search_terms, \"wb\"))\n",
    "pickle.dump(comment_list, open(\"support/%s/comment_list.pkl\" % search_terms, \"wb\"))\n",
    "pickle.dump(vid_title, open(\"support/%s/vid_title.pkl\" % search_terms, \"wb\"))\n",
    "pickle.dump(vid_page, open(\"support/%s/vid_page.pkl\" % search_terms, \"wb\"))\n",
    "pickle.dump(vid_id, open(\"support/%s/vid_id.pkl\" % search_terms, \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save files for next step \"\"\"\n",
    "import shutil\n",
    "\n",
    "source = \"support/%s/comments.txt\" % search_terms\n",
    "destination = \"support/_current_/comments.txt\"\n",
    "shutil.copyfile(source, destination)\n",
    "\n",
    "pickle.dump(search_terms, open(\"support/_current_/searchTerms.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2.2: Reddit Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dear user, define the subreddit for the product. \n",
    "\"\"\"\n",
    "subreddit = \"r/virtualreality\"              # Define the subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Initialise and Set up Reddit API \"\"\"\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "id = os.getenv(\"REDDIT_API_ID\")\n",
    "key = os.getenv(\"REDDIT_API_KEY\")\n",
    "user = os.getenv(\"REDDIT_API_USER\")\n",
    "pw = os.getenv(\"REDDIT_API_PW\")\n",
    "\n",
    "auth = requests.auth.HTTPBasicAuth(id, key)\n",
    "\n",
    "data = {'grant_type': 'password',                                       # Initalize using login method (password), username, and password\n",
    "        'username': user,\n",
    "        'password': pw}\n",
    "\n",
    "headers = {'User-Agent': 'DAI/AID'}                                     # Setup our header info, which gives reddit a brief description of our app\n",
    "\n",
    "res = requests.post('https://www.reddit.com/api/v1/access_token',       # Send request for an OAuth token\n",
    "                    auth=auth, data=data, headers=headers)\n",
    "\n",
    "TOKEN = res.json()['access_token']                                      # Convert response to JSON and pull access_token value\n",
    "\n",
    "headers = {**headers, **{'Authorization': f\"bearer {TOKEN}\"}}\n",
    "\n",
    "requests.get('https://oauth.reddit.com/api/v1/me', headers=headers)     # Token is valid for ~2 hours. If get Response [200], you are good to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define containers to store info \"\"\"\n",
    "post_id = []             \t     # post id\n",
    "post_page = []       \t\t     # post links (https...)\n",
    "post_title = []                  # post title\n",
    "post_num_comments = []           # official number of comments\n",
    "post_comment_list = []           # temp. list for storing comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Search for Post IDs based on User Inputs \"\"\"\n",
    "print(\"Search for Post IDs...\")\n",
    "res = requests.get(f\"https://oauth.reddit.com/{subreddit}/search/?q={search_terms}&restrict_sr=on&sort=relevance&t=all\",   # Restrict search to only r/virtualreality\n",
    "                   headers=headers)\n",
    "\n",
    "if res.status_code == 200:                  # Check if the request was successful\n",
    "    search_response = res.json()            # Parse the response as JSON\n",
    "    print(search_response)\n",
    "else:\n",
    "    print(f\"Error: {res.status_code} - {res.reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create a list of Post IDs and a corresponding list of weblinks \"\"\"\n",
    "print(\"Posts found...\")\n",
    "\n",
    "for post in search_response['data']['children']:\n",
    "    postId = post['data']['id']\n",
    "    print(postId)\n",
    "    post_id.append(postId)                           # a list of Post IDs\n",
    "    page = f\"https://oauth.reddit.com/r/virtualreality/comments/{postId}\"\n",
    "    print(page)\n",
    "    print()\n",
    "    post_page.append(page)                           # a list of Post links\n",
    "\n",
    "print(f\"\\nThere are {len(post_page)} posts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Use the list of Post IDs to get post data \"\"\"\n",
    "print(\"Get post data...\")\n",
    "for i, post_response in enumerate(search_response['data']['children']):\n",
    "    try:\n",
    "        print(post_response)\n",
    "        title = post_response['data']['title']                                  # Extract title and comment count from post response\n",
    "        post_title.append(title)\n",
    "        comment_count = post_response['data']['num_comments']\n",
    "        print(f\"Post {i + 1} - {title} -- Comment count: {comment_count}\")\n",
    "        post_num_comments.append(comment_count)\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching post data: {e}\")\n",
    "\n",
    "print(sum(post_num_comments), \"comments in total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Sift through and store comments as a list \"\"\"\n",
    "print(\"Get individual comment...\")\n",
    "\n",
    "def scrape_comments(url, comment_list):\n",
    "    try:\n",
    "        page_json = f\"{url}.json\"\n",
    "\n",
    "        post_comments_response = requests.get(page_json, headers=headers) \n",
    "        if post_comments_response.status_code == 200:                                       # Check if request was successful\n",
    "            post_comments_data = post_comments_response.json()\n",
    "            if isinstance(post_comments_data, list) and len(post_comments_data) > 1:\n",
    "                total_comments = post_comments_data[1]['data']['children'][0]['data']['total_awards_received']\n",
    "                print(f\"Total comments: {total_comments}\")\n",
    "                comments = post_comments_data[1]['data']['children']                        # Extract comments from the response\n",
    "                for comment in comments:\n",
    "                    try:\n",
    "                        comment_body = comment['data']['body']\n",
    "                        print(comment_body)\n",
    "                        comment_list.append(comment_body)\n",
    "                        if 'replies' in comment['data'] and comment['data']['replies']:     # Check for replies and recursively scrape them\n",
    "                            scrape_replies(comment['data']['replies']['data']['children'], comment_list)\n",
    "                    except KeyError:\n",
    "                        print(\"Error: Missing 'body' attribute for a comment.\")\n",
    "        else:\n",
    "            print(f\"Error fetching comments for page {url}: {post_comments_response.status_code} - {post_comments_response.reason}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching comments for page {url}:\", e)\n",
    "\n",
    "def scrape_replies(replies, comment_list):                                                  # Function to scrape replies recursively\n",
    "    for reply in replies:\n",
    "        try:\n",
    "            reply_body = reply['data']['body']\n",
    "            print(reply_body)\n",
    "            comment_list.append(reply_body)\n",
    "            # Recursively scrape replies of replies\n",
    "            if 'replies' in reply['data'] and reply['data']['replies']:\n",
    "                scrape_replies(reply['data']['replies']['data']['children'], comment_list)\n",
    "        except KeyError:\n",
    "            print(\"Error: Missing 'body' attribute for a reply.\")\n",
    "\n",
    "for page in post_page:\n",
    "    scrape_comments(page, post_comment_list)\n",
    "\n",
    "print(post_comment_list)\n",
    "print()\n",
    "print(len(post_comment_list), \"comments in total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create directory \"\"\"\n",
    "try:                                              # Create directory named after search terms\n",
    "    os.makedirs(\"support/%s\" % search_terms)\n",
    "    print(\"Directory\", search_terms, \"created\")\n",
    "except FileExistsError:\n",
    "    print(\"Directory\", search_terms, \"exists\")\n",
    "\n",
    "try:                                              # Create directory to store current search terms\n",
    "    os.makedirs(\"support/_current_\")\n",
    "    print(\"Directory _current_ created\")\n",
    "except FileExistsError:\n",
    "    print(\"Directory _current_ exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save files for future use \"\"\"\n",
    "import pickle\n",
    "\n",
    "f = open(\"support/%s/post_comments.txt\" % search_terms, \"w+\", encoding=\"utf-8\")\n",
    "for i in range(len(post_comment_list)):\n",
    "    f.write(\"<<<\" + post_comment_list[i] + \">>>\")\n",
    "f.close()\n",
    "\n",
    "pickle.dump(search_terms, open(\"support/%s/searchTerms.pkl\" % search_terms, \"wb\"))\n",
    "pickle.dump(post_comment_list, open(\"support/%s/post_comment_list.pkl\" % search_terms, \"wb\"))\n",
    "pickle.dump(post_title, open(\"support/%s/post_title.pkl\" % search_terms, \"wb\"))\n",
    "pickle.dump(post_page, open(\"support/%s/post_page.pkl\" % search_terms, \"wb\"))\n",
    "pickle.dump(post_id, open(\"support/%s/post_id.pkl\" % search_terms, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save files for next step \"\"\"\n",
    "import shutil\n",
    "\n",
    "source = \"support/%s/post_comments.txt\" % search_terms\n",
    "destination = \"support/_current_/post_comments.txt\"\n",
    "shutil.copyfile(source, destination)\n",
    "\n",
    "pickle.dump(search_terms, open(\"support/_current_/searchTerms.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2.3: Official Product Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dear user, please enter the URL of the website and the container containing what you wish to scrape.\n",
    "\"\"\"\n",
    "url = \"https://www.picoxr.com/sg/products/pico4\"\n",
    "container_tag = \"main\"\n",
    "container_class = \"tIY88xTQJtQ9ZOGrUS1U\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:                                     # Check if the request was successful\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')           # Parse the HTML content\n",
    "\n",
    "    title_tag = soup.find(\"title\")                                  # Get the title of the webpage if it exists\n",
    "    if title_tag:\n",
    "        title = title_tag.get_text()\n",
    "        print(\"Title:\", title)\n",
    "    else:\n",
    "        print(\"No title found\")\n",
    "\n",
    "    product_desc = \"\"\n",
    "\n",
    "    text = soup.find_all(container_tag, class_=container_class)     # Get text from the container containing the product description\n",
    "    for i in range(len(text)):\n",
    "        text[i] = re.sub(r'\\<.*?\\>', ' ', str(text[i]))\n",
    "        text[i] = text[i].replace('\\n', ' ')\n",
    "        text[i] = text[i].replace(\"   \", ' ')\n",
    "        text[i] = text[i].replace(\"  \", ' ')\n",
    "        text[i] = re.sub(r'\\s+', ' ', text[i].strip())\n",
    "    product_desc = \" \".join(text)\n",
    "\n",
    "    print(\"Product Description:\", product_desc)\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve webpage. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create directory \"\"\"\n",
    "try:                                              # Create directory named after search terms\n",
    "    os.makedirs(\"support/%s\" % search_terms)\n",
    "    print(\"Directory\", search_terms, \"created\")\n",
    "except FileExistsError:\n",
    "    print(\"Directory\", search_terms, \"exists\")\n",
    "\n",
    "try:                                              # Create directory to store current search terms\n",
    "    os.makedirs(\"support/_current_\")\n",
    "    print(\"Directory _current_ created\")\n",
    "except FileExistsError:\n",
    "    print(\"Directory _current_ exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save files for future use \"\"\"\n",
    "f = open(\"support/%s/product_desc.txt\" % search_terms, \"w+\", encoding=\"utf-8\")\n",
    "f.write(\"<<<\" + product_desc + \">>>\")\n",
    "f.close()\n",
    "\n",
    "pickle.dump(search_terms, open(\"support/%s/searchTerms.pkl\" % search_terms, \"wb\"))\n",
    "pickle.dump(product_desc, open(\"support/%s/product_desc.pkl\" % search_terms, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save files for next step \"\"\"\n",
    "import shutil\n",
    "\n",
    "source = \"support/%s/product_desc.txt\" % search_terms\n",
    "destination = \"support/_current_/product_desc.txt\"\n",
    "shutil.copyfile(source, destination)\n",
    "\n",
    "pickle.dump(search_terms, open(\"support/_current_/searchTerms.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2.4 Tech Magazine Review (PCGamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dear user, please enter the URL of the website.\n",
    "\"\"\"\n",
    "url = \"https://www.pcgamer.com/pico-4-ve-headset-review/\"\n",
    "container_tag = \"div\"\n",
    "container_class = \"text-copy bodyCopy auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:                                     # Check if the request was successful\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')           # Parse the HTML content\n",
    "\n",
    "    title_tag = soup.find(\"title\")                                  # Get the title of the webpage if it exists\n",
    "    if title_tag:\n",
    "        title = title_tag.get_text()\n",
    "        print(\"Title:\", title)\n",
    "    else:\n",
    "        print(\"No title found\")\n",
    "\n",
    "    pc_gamer_review = \"\"\n",
    "\n",
    "    text = soup.find_all(container_tag, class_=container_class)              # Get text from the container containing the product description\n",
    "    for i in range(len(text)):\n",
    "        text[i] = re.sub(r'\\<script.*?\\<\\/script\\>', '', str(text[i]), flags=re.DOTALL)\n",
    "        text[i] = re.sub(r'\\<.*?\\>', ' ', str(text[i]))\n",
    "        text[i] = text[i].replace('\\n', ' ')\n",
    "        text[i] = text[i].replace(\"   \", ' ')\n",
    "        text[i] = text[i].replace(\"  \", ' ')\n",
    "        text[i] = re.sub(r'\\s+', ' ', text[i].strip())\n",
    "    pc_gamer_review = \" \".join(text)\n",
    "\n",
    "    print(\"PC Gamer Review:\", pc_gamer_review)\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve webpage. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create directory \"\"\"\n",
    "try:                                              # Create directory named after search terms\n",
    "    os.makedirs(\"support/%s\" % search_terms)\n",
    "    print(\"Directory\", search_terms, \"created\")\n",
    "except FileExistsError:\n",
    "    print(\"Directory\", search_terms, \"exists\")\n",
    "\n",
    "try:                                              # Create directory to store current search terms\n",
    "    os.makedirs(\"support/_current_\")\n",
    "    print(\"Directory _current_ created\")\n",
    "except FileExistsError:\n",
    "    print(\"Directory _current_ exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save files for future use \"\"\"\n",
    "f = open(\"support/%s/pc_gamer_review.txt\" % search_terms, \"w+\", encoding=\"utf-8\")\n",
    "f.write(\"<<<\" + pc_gamer_review + \">>>\")\n",
    "f.close()\n",
    "\n",
    "pickle.dump(search_terms, open(\"support/%s/searchTerms.pkl\" % search_terms, \"wb\"))\n",
    "pickle.dump(pc_gamer_review, open(\"support/%s/pc_gamer_review.pkl\" % search_terms, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save files for next step \"\"\"\n",
    "import shutil\n",
    "\n",
    "source = \"support/%s/pc_gamer_review.txt\" % search_terms\n",
    "destination = \"support/_current_/pc_gamer_review.txt\"\n",
    "shutil.copyfile(source, destination)\n",
    "\n",
    "pickle.dump(search_terms, open(\"support/_current_/searchTerms.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2.3.5 Official User Manual (PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import requests\n",
    "import os\n",
    "\n",
    "\"\"\"\n",
    "Dear user, please enter the URL of the PDF file.\n",
    "\"\"\"\n",
    "url = 'https://pico-web-tob.oss-cn-beijing.aliyuncs.com/20230825/document/1695015503416348672.pdf'\n",
    "\n",
    "def extract_text_from_pdf(url):\n",
    "    response = requests.get(url)\n",
    "    with open('user_manual.pdf', 'wb') as f:                    # Download the PDF file\n",
    "        f.write(response.content)\n",
    "\n",
    "    with open('user_manual.pdf', 'rb') as f:                    # Open the PDF file\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        \n",
    "        text = ''\n",
    "        for page_number in range(2, 11):\n",
    "            page = reader.pages[page_number]\n",
    "            text += \"\".join(page.extract_text())                # Extract text from each page\n",
    "    \n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    return text\n",
    "\n",
    "user_manual_text = extract_text_from_pdf(url)\n",
    "os.remove('user_manual.pdf')\n",
    "\n",
    "print(user_manual_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create directory \"\"\"\n",
    "try:                                              # Create directory named after search terms\n",
    "    os.makedirs(\"support/%s\" % search_terms)\n",
    "    print(\"Directory\", search_terms, \"created\")\n",
    "except FileExistsError:\n",
    "    print(\"Directory\", search_terms, \"exists\")\n",
    "\n",
    "try:                                              # Create directory to store current search terms\n",
    "    os.makedirs(\"support/_current_\")\n",
    "    print(\"Directory _current_ created\")\n",
    "except FileExistsError:\n",
    "    print(\"Directory _current_ exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save files for future use \"\"\"\n",
    "f = open(\"support/%s/user_manual.txt\" % search_terms, \"w+\", encoding=\"utf-8\")\n",
    "f.write(\"<<<\" + user_manual_text + \">>>\")\n",
    "f.close()\n",
    "\n",
    "pickle.dump(search_terms, open(\"support/%s/searchTerms.pkl\" % search_terms, \"wb\"))\n",
    "pickle.dump(user_manual_text, open(\"support/%s/user_manual.pkl\" % search_terms, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save files for next step \"\"\"\n",
    "import shutil\n",
    "\n",
    "source = \"support/%s/user_manual.txt\" % search_terms\n",
    "destination = \"support/_current_/user_manual.txt\"\n",
    "shutil.copyfile(source, destination)\n",
    "\n",
    "pickle.dump(search_terms, open(\"support/_current_/searchTerms.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
