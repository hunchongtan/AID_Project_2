{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Benchmarking using Selective Classified Sentiment Analysis\n",
    "\n",
    "Dear user, we will be conducting benchmarking for our identified components, \\\n",
    "where we scrap Youtube comments from other competitors, and find out if the components from other competitors fair better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dear user, enter your Product here!\n",
    "'''\n",
    "product = \"Boeing 787 Dreamliner Commercial Plane\"\n",
    "\n",
    "search_terms = product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dear user, enter 2 competitors here!\n",
    "'''\n",
    "competitor_1 = \"Airbus 320 Commercial Plane\"\n",
    "competitor_2 = \"Embraer 190 Commercial Plane\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Initialise and Set up Google API Key \"\"\"\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "youtube = build('youtube', 'v3', developerKey=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching comments: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet%2Creplies&videoId=zE88Sz2t_gE&key=AIzaSyA5lBkdcbGjl_Fe2xQd0GJJ29ZwTJnJrnI&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching comments: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet%2Creplies&videoId=zE88Sz2t_gE&key=AIzaSyA5lBkdcbGjl_Fe2xQd0GJJ29ZwTJnJrnI&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Videos for Airbus 320 Commercial Plane\n",
      "1 Boeing Vietnam Airlines 787-9 Dreamliner Vertical Takeoff &amp; Steep Turns 2015 Paris Air Show Prep\n",
      "2 Boeing 787-10 Dreamliner and 737 MAX 9 Fly Together in Dramatic Display\n",
      "3 The Insane Engineering of the 787\n",
      "4 Boeing 787 Dreamliner soars for first flight\n",
      "5 United — This is the story of new planes\n",
      "\n",
      "Videos for Embraer 190 Commercial Plane\n",
      "1 Boeing Vietnam Airlines 787-9 Dreamliner Vertical Takeoff &amp; Steep Turns 2015 Paris Air Show Prep\n",
      "2 Boeing 787-10 Dreamliner and 737 MAX 9 Fly Together in Dramatic Display\n",
      "3 The Insane Engineering of the 787\n",
      "4 Boeing 787 Dreamliner soars for first flight\n",
      "5 United — This is the story of new planes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def youtube_scrap(competitor):\n",
    "    max_results = 5\n",
    "    vid_id = []             \t# video id\n",
    "    vid_page = []       \t\t# video links (https...)\n",
    "    vid_title = []              # video title\n",
    "    num_comments = []           # official number of comments\n",
    "    comment_list = []           # temp. list for storing comments\n",
    "    comment_resp = []           # comment_response\n",
    "\n",
    "    request = youtube.search().list(\n",
    "        q=search_terms,\n",
    "        maxResults=max_results,\n",
    "        part=\"id,snippet\",  # Include snippet in the request\n",
    "        type=\"video\",\n",
    "        order=\"relevance\"         # Switch to \"viewCount\" if the number of comments are not sufficient\n",
    "    )\n",
    "    search_response = request.execute()\n",
    "\n",
    "    for item in search_response.get('items', []):\n",
    "        if 'snippet' in item:\n",
    "            vid_id.append(item['id']['videoId'])\n",
    "            vid_title.append(item['snippet']['title'])\n",
    "            page = \"https://www.youtube.com/watch?v=\" + item['id']['videoId']\n",
    "            vid_page.append(page)\n",
    "\n",
    "    for video_id, title in zip(vid_id, vid_title):\n",
    "        request = youtube.videos().list(\n",
    "            part=\"snippet, statistics\",\n",
    "            id=video_id\n",
    "        )\n",
    "        video_response = request.execute()\n",
    "        num_comments.append(video_response['items'][0]['statistics'].get('commentCount', 0))\n",
    "\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet,replies\",\n",
    "                videoId=video_id\n",
    "            )\n",
    "            comment_response = request.execute()\n",
    "            comment_resp.append(comment_response)\n",
    "        except Exception as e:\n",
    "            print(\"Error fetching comments:\", e)\n",
    "\n",
    "    for response in comment_resp:\n",
    "        for item in response.get('items', []):\n",
    "            try:\n",
    "                comment_list.append(item['snippet']['topLevelComment']['snippet']['textDisplay'])\n",
    "            except Exception as e:\n",
    "                print(\"Error processing comment:\", e)\n",
    "\n",
    "    # Save comment_list to a file\n",
    "    directory = \"support/%s/competitor/%s\" % (search_terms, competitor)\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    with open(os.path.join(directory, \"comment_list.txt\"), \"w+\", encoding=\"utf-8\") as f:\n",
    "        for comment in comment_list:\n",
    "            f.write(\"<<<\" + comment + \">>>\")\n",
    "\n",
    "    with open(os.path.join(directory, \"comment_list.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(comment_list, f)\n",
    "\n",
    "    return vid_title, comment_list  # Return video titles and comments\n",
    "\n",
    "# Call the function for each competitor\n",
    "videos_1, comments_1 = youtube_scrap(competitor_1)\n",
    "videos_2, comments_2 = youtube_scrap(competitor_2)\n",
    "\n",
    "# Print list of videos\n",
    "print(\"Videos for\", competitor_1)\n",
    "for i, title in enumerate(videos_1, start=1):\n",
    "    print(i, title)\n",
    "\n",
    "print(\"\\nVideos for\", competitor_2)\n",
    "for i, title in enumerate(videos_2, start=1):\n",
    "    print(i, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "def initiate_vector_store(setting):\n",
    "    \"\"\" Combine text files into one \"\"\"\n",
    "    with open(\"combined.txt\", \"w\", encoding=\"utf-8\") as combined_file:\n",
    "        for file_path in setting:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "                combined_file.write(content)\n",
    "    \n",
    "    \"\"\" Load Private Documents of User Manual \"\"\"\n",
    "    loader = TextLoader(file_path=\"combined.txt\", encoding='utf-8')\n",
    "    document = loader.load()\n",
    "    os.remove('combined.txt')\n",
    "\n",
    "    \"\"\" Split Documents into smaller parts \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "    splits = text_splitter.split_documents(document)\n",
    "\n",
    "    \"\"\" Use OpenAI Embeddings \"\"\"\n",
    "    embedding = OpenAIEmbeddings()\n",
    "\n",
    "    \"\"\" Remove 'persist' directory, if any \"\"\"\n",
    "    try:\n",
    "        shutil.rmtree('support/%s/competitor/persist' % search_terms)       # remove old version\n",
    "        print(\"Deleting previous store\")\n",
    "    except:\n",
    "        print(\"No store found\")\n",
    "\n",
    "    persist_directory = 'support/%s/competitor/persist' % search_terms     # create new version\n",
    "\n",
    "    \"\"\" Apply embeddings on private documents and save in 'persist' directory \"\"\"\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=splits,                           # target the splits created from the documents loaded\n",
    "        embedding=embedding,                        # use the OpenAI embedding specified\n",
    "        persist_directory=persist_directory         # store in the persist directory for future use\n",
    "    )\n",
    "\n",
    "    vectordb.persist()                              # store vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "def private_query(competitor):\n",
    "    \"\"\" Retrieve vectordb created \"\"\"\n",
    "    embedding = OpenAIEmbeddings()\n",
    "    persist_directory = 'support/%s/persist' % search_terms\n",
    "\n",
    "    vectordb = Chroma(\n",
    "        persist_directory=persist_directory,\n",
    "        embedding_function=embedding\n",
    "        )\n",
    "\n",
    "    print(\"Processing folder:\", search_terms)\n",
    "    print(\"Size of Vector Database\", vectordb._collection.count())    # same as before\n",
    "    \n",
    "    \"\"\" Apply language model and vectordb for Chatbot \"\"\"\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        # \"similarity\" finds similar items based on similarity metric.\n",
    "        # search_kwargs: number of similar items (k) to retrieve\n",
    "        retriever=vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 15}),     \n",
    "        return_source_documents=True\n",
    "        )\n",
    "    \n",
    "    \"\"\" Ready to use GPT \"\"\"\n",
    "    question = \" '''What is the airline discussed? Is the sentiment on the service mainly positive or negative? What is a key negative point?''' \"  # input question\n",
    "    template = \" If you don't know the answer to the question delimited by triple quotes based on the data given, strictly state 'I don't know'. Keep the answer as concise as possible, with a maximum of three sentences.\"\n",
    "\n",
    "    prompt = question + template\n",
    "    result = qa_chain({\"query\": prompt})\n",
    "\n",
    "    components = result[\"result\"]\n",
    "    print(\"Components:\", components)\n",
    "\n",
    "    pickle.dump(components, open(\"support/%s/DSM/%s.pkl\" % (search_terms, number), \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
