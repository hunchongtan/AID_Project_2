{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Design Structure Matrix\n",
    "\n",
    "Dear user, given input data from different sources, \\\n",
    "we will query a LLM to help us output components of the product \\\n",
    "to construct design structure matrixes for further design analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REQUIREMENTS\n",
    "\n",
    "For this notebook, you need to have:\n",
    "- 5 x Text files of Scraped data from Online sources (from Chap2.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dear user, enter your Product here!\n",
    "'''\n",
    "\n",
    "product = \"Boeing 787 Dreamliner Commercial Plane\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dear user, enter your directories to the 5 txt files of Scraped data here!\n",
    "'''\n",
    "youtube = f\"support/{product}/youtube/comment_list.txt\"\n",
    "reddit = f\"support/{product}/reddit/comment_list.txt\"\n",
    "wikipedia = f\"support/{product}/wikipedia/wikipedia.txt\"\n",
    "brochure = f\"support/{product}/brochure/brochure.txt\"\n",
    "textbook = f\"support/{product}/textbook/textbook.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dear user, please input different input settings to generate the best Design Structure Matrix!\n",
    "For comment lists, use ''.join(comment_list) to convert the list to a string.\n",
    "'''\n",
    "setting_1 = [youtube, reddit]\n",
    "setting_2 = [brochure, wikipedia]\n",
    "setting_3 = [textbook, wikipedia]\n",
    "setting_4 = [textbook, brochure, youtube]\n",
    "setting_5 = [textbook, youtube, reddit]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN AS INTENDED (DO NOT CHANGE ANYTHING.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain==0.1.9 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.1.9)\n",
      "Requirement already satisfied: chromadb==0.4.24 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.4.24)\n",
      "Requirement already satisfied: langchain-openai==0.0.8 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.0.8)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langchain==0.1.9) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langchain==0.1.9) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langchain==0.1.9) (3.8.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langchain==0.1.9) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langchain==0.1.9) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langchain==0.1.9) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.21 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langchain==0.1.9) (0.0.29)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.26 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langchain==0.1.9) (0.1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langchain==0.1.9) (0.1.31)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langchain==0.1.9) (1.25.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langchain==0.1.9) (2.3.0)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langchain==0.1.9) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langchain==0.1.9) (8.2.3)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from chromadb==0.4.24) (1.1.1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from chromadb==0.4.24) (0.7.3)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from chromadb==0.4.24) (0.102.0)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.24) (0.23.2)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from chromadb==0.4.24) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from chromadb==0.4.24) (4.7.1)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from chromadb==0.4.24) (3.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from chromadb==0.4.24) (1.17.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from chromadb==0.4.24) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from chromadb==0.4.24) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from chromadb==0.4.24) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from chromadb==0.4.24) (1.23.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from chromadb==0.4.24) (0.13.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from chromadb==0.4.24) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from chromadb==0.4.24) (4.66.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from chromadb==0.4.24) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from chromadb==0.4.24) (6.0.1)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from chromadb==0.4.24) (1.62.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from chromadb==0.4.24) (4.1.2)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from chromadb==0.4.24) (0.10.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from chromadb==0.4.24) (29.0.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from chromadb==0.4.24) (4.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from chromadb==0.4.24) (3.9.15)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.10.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langchain-openai==0.0.8) (1.13.3)\n",
      "Requirement already satisfied: tiktoken<1,>=0.5.2 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langchain-openai==0.0.8) (0.6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.9) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.9) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.9) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.9) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.9) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.9) (1.3.1)\n",
      "Requirement already satisfied: packaging>=19.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from build>=1.0.3->chromadb==0.4.24) (23.2)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from build>=1.0.3->chromadb==0.4.24) (1.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from build>=1.0.3->chromadb==0.4.24) (0.4.6)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from build>=1.0.3->chromadb==0.4.24) (6.8.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from build>=1.0.3->chromadb==0.4.24) (2.0.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.9) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.9) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from fastapi>=0.95.2->chromadb==0.4.24) (0.27.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.9) (2.4)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.24) (2023.7.22)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.24) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.24) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.24) (2.15.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.24) (1.5.1)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.24) (1.3.1)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.24) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.24) (2.0.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langchain-core<0.2,>=0.1.26->langchain==0.1.9) (3.7.1)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.24) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.24) (2.0.7)\n",
      "Requirement already satisfied: protobuf in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.24) (3.19.6)\n",
      "Requirement already satisfied: sympy in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.24) (1.12)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.8) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.8) (0.24.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.8) (1.3.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb==0.4.24) (1.2.14)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.24) (1.59.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.23.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.24) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.23.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.24) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.44b0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.24) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.44b0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.24) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.44b0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.24) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.44b0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.24) (0.44b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in c:\\users\\tanhu\\appdata\\roaming\\python\\python39\\site-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.24) (68.2.2)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.24) (1.14.1)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.24) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from posthog>=2.4.0->chromadb==0.4.24) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from posthog>=2.4.0->chromadb==0.4.24) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pydantic<3,>=1->langchain==0.1.9) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.6.3 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pydantic<3,>=1->langchain==0.1.9) (2.6.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2->langchain==0.1.9) (3.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.9) (3.0.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tiktoken<1,>=0.5.2->langchain-openai==0.0.8) (2023.8.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from typer>=0.9.0->chromadb==0.4.24) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.4.24) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.24) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.24) (1.0.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.24) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.24) (11.0.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from importlib-resources->chromadb==0.4.24) (3.17.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.26->langchain==0.1.9) (1.1.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.24) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.24) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.24) (4.9)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai==0.0.8) (0.17.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.9) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.24) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.4.24) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.24) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\tanhu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.24) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain==0.1.9 chromadb==0.4.24 langchain-openai==0.0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Set up OpenAI API key \"\"\"\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create DSM folder \"\"\"\n",
    "import os\n",
    "import shutil\n",
    "try:\n",
    "    os.makedirs(\"support/%s/%s\" % (search_terms, \"DSM\"))\n",
    "except FileExistsError:\n",
    "    shutil.rmtree('support/%s/DSM' % search_terms)\n",
    "    os.makedirs(\"support/%s/%s\" % (search_terms, \"DSM\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Store Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "def initiate_vector_store(setting):\n",
    "    \"\"\" Combine text files into one \"\"\"\n",
    "    with open(\"combined.txt\", \"w\", encoding=\"utf-8\") as combined_file:\n",
    "        for file_path in setting:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "                combined_file.write(content)\n",
    "    \n",
    "    \"\"\" Load Private Documents of User Manual \"\"\"\n",
    "    loader = TextLoader(file_path=\"combined.txt\", encoding='utf-8')\n",
    "    document = loader.load()\n",
    "    os.remove('combined.txt')\n",
    "\n",
    "    \"\"\" Split Documents into smaller parts \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=150\n",
    "    )\n",
    "    splits = text_splitter.split_documents(document)\n",
    "\n",
    "    \"\"\" Use OpenAI Embeddings \"\"\"\n",
    "    embedding = OpenAIEmbeddings()\n",
    "\n",
    "    \"\"\" Remove 'persist' directory, if any \"\"\"\n",
    "    try:\n",
    "        shutil.rmtree('support/%s/persist' % search_terms)       # remove old version\n",
    "        print(\"Deleting previous store\")\n",
    "    except:\n",
    "        print(\"No store found\")\n",
    "\n",
    "    persist_directory = 'support/%s/persist' % search_terms     # create new version\n",
    "\n",
    "    \"\"\" Apply embeddings on private documents and save in 'persist' directory \"\"\"\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=splits,                           # target the splits created from the documents loaded\n",
    "        embedding=embedding,                        # use the OpenAI embedding specified\n",
    "        persist_directory=persist_directory         # store in the persist directory for future use\n",
    "    )\n",
    "\n",
    "    vectordb.persist()                              # store vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "def private_query(setting, number):\n",
    "    \"\"\" Retrieve vectordb created \"\"\"\n",
    "    embedding = OpenAIEmbeddings()\n",
    "    persist_directory = 'support/%s/persist' % search_terms\n",
    "\n",
    "    vectordb = Chroma(\n",
    "        persist_directory=persist_directory,\n",
    "        embedding_function=embedding\n",
    "        )\n",
    "\n",
    "    print(\"Processing folder:\", search_terms)\n",
    "    print(\"Size of Vector Database\", vectordb._collection.count())    # same as before\n",
    "    \n",
    "    \"\"\" Apply language model and vectordb for Chatbot \"\"\"\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        # MMR focuses on balancing relevance and diversity.\n",
    "        # \"k\": the total number of documents to retrieve.\n",
    "        # \"fetch_k\": number of documents to fetch from the database for each round of selection.\n",
    "        retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 20, \"fetch_k\": 5}),      \n",
    "        return_source_documents=True\n",
    "        )\n",
    "    \n",
    "    \"\"\" Ready to use GPT \"\"\"\n",
    "    question = f\"Strictly based on the dataset, identify ten different main components that make up {search_terms}.\"\n",
    "    template = \" Express the answer only as a Python list with strictly a non-descriptive physical component (do not add numbering). Do not output anything else. If you don't know the answer, strictly state 'I don't know' instead of making up an answer.\"\n",
    "\n",
    "    prompt = question + template\n",
    "    result = qa_chain({\"query\": prompt})\n",
    "\n",
    "    components = result[\"result\"]\n",
    "    print(\"Components:\", components)\n",
    "\n",
    "    pickle.dump(components, open(\"support/%s/DSM/%s.pkl\" % (search_terms, number), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Query for Components Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting 1\n",
      "Deleting previous store\n",
      "Processing folder: Boeing 787 Dreamliner Commercial Plane\n",
      "Size of Vector Database 1393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanhu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components: ['Carbon fiber reinforced plastic', 'Chevrons', 'Dimmable windows', 'Wings', 'Engines', 'Titanium parts', 'Safety caps', 'Copper wires', 'Platinum material', 'Battery']\n",
      "\n",
      "Setting 2\n",
      "No store found\n",
      "Processing folder: Boeing 787 Dreamliner Commercial Plane\n",
      "Size of Vector Database 1754\n",
      "Components: ['composite structures', 'aluminum', 'titanium', 'steel', 'engines', 'wings', 'tailplanes', 'nose contour', 'wingtips', 'engine nacelles']\n",
      "\n",
      "Setting 3\n",
      "No store found\n",
      "Processing folder: Boeing 787 Dreamliner Commercial Plane\n",
      "Size of Vector Database 2907\n",
      "Components: ['composite', 'aluminum', 'titanium', 'steel', 'engines', 'fasteners', 'nose contour', 'wingtips', 'engine nacelles', 'barrel sections']\n",
      "\n",
      "Setting 4\n",
      "No store found\n",
      "Processing folder: Boeing 787 Dreamliner Commercial Plane\n",
      "Size of Vector Database 4748\n",
      "Components: ['composite', 'aluminum', 'titanium', 'steel', 'other materials', 'engines', 'fuselage', 'wings', 'tailplanes', 'fasteners']\n",
      "\n",
      "Setting 5\n",
      "No store found\n",
      "Processing folder: Boeing 787 Dreamliner Commercial Plane\n",
      "Size of Vector Database 7081\n",
      "Components: ['composite', 'aluminum', 'titanium', 'steel', 'other materials', 'engines', 'wingtips', 'nacelles', 'leading edges', 'tailplanes']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "settings = [setting_1, setting_2, setting_3, setting_4, setting_5]\n",
    "\n",
    "for i in range(len(settings)):\n",
    "    setting = settings[i]\n",
    "    setting_no = \"Setting \" + str(i+1)\n",
    "    print(setting_no)\n",
    "    initiate_vector_store(setting)\n",
    "    private_query(setting, setting_no)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto DSM from Components Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "import pickle\n",
    "\n",
    "def empty_csv_matrix(components, filename):\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        \n",
    "        # Write headers\n",
    "        writer.writerow([''] + components)  # Empty header for the first cell, then components\n",
    "        \n",
    "        # Write empty rows with component headers\n",
    "        for i, component in enumerate(components):\n",
    "            row = [component] + ['X' if i == j else '' for j in range(10)]  # 'X' for diagonal cells, otherwise empty string\n",
    "            writer.writerow(row)\n",
    "\n",
    "\"\"\" Create empty CSV matrices for each setting \"\"\"\n",
    "setnames = ['setting_1', 'setting_2', 'setting_3', 'setting_4', 'setting_5']\n",
    "\n",
    "for i in range(len(setnames)):\n",
    "    with open(\"support/%s/DSM/Setting %s.pkl\" % (search_terms, i+1), \"rb\") as f:\n",
    "        componentstr = pickle.load(f)\n",
    "        components = ast.literal_eval(componentstr)\n",
    "\n",
    "    filename = \"support/%s/DSM/%s.csv\" % (search_terms, setnames[i])\n",
    "\n",
    "    empty_csv_matrix(components, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import csv\n",
    "\n",
    "def auto_DSM(components, setname):\n",
    "    with open(\"support/%s/DSM/%s.csv\" % (search_terms, setname), 'r', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        matrix = list(reader)\n",
    "    \n",
    "    for i, component_row in enumerate(matrix[1:]):\n",
    "        for j, _ in enumerate(component_row[1:]):\n",
    "            if i == j:\n",
    "                continue  # Skip diagonal cells\n",
    "            element_a = components[i]\n",
    "            element_b = components[j]\n",
    "            linkage_type = \"mechanically\"             ### (Adjustable) Specify the linkage type\n",
    "\n",
    "            client = OpenAI()\n",
    "            \n",
    "            question = f\"Are {element_a} and {element_b} {linkage_type} linked? State 'Yes' or 'No' or 'Unsure'.\"\n",
    "            template = \" Do not output with full stops. If you don't know the answer, strictly state 'Unsure' instead of making up an answer.\"\n",
    "            prompt = question + template\n",
    "            \n",
    "            chat_completion = client.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.5,\n",
    "            )\n",
    "            result = chat_completion.choices[0].message.content\n",
    "            \n",
    "            matrix[i+1][j+1] = result\n",
    "    \n",
    "    \"\"\" Write the filled matrix back to CSV \"\"\"\n",
    "    with open(\"support/%s/DSM/%s.csv\" % (search_terms, setname), 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for row in matrix:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\"\"\" Auto-fill DSM \"\"\"\n",
    "setnames = ['setting_1', 'setting_2', 'setting_3', 'setting_4', 'setting_5']\n",
    "\n",
    "for setname in setnames:\n",
    "    auto_DSM(components, setname)\n",
    "    print(\"DSM for\", setname, \"is complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design Structure Matrixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "def DSM_display(csv_file, setname):\n",
    "    df = pd.read_csv(csv_file, index_col=0)\n",
    "\n",
    "    df.replace({'Yes': 1, 'No': 0, 'Unsure': 2, 'X': np.nan}, inplace=True)\n",
    "\n",
    "    matrix = df.values\n",
    "    row_headers = df.index\n",
    "    col_headers = df.columns\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(matrix, cmap='binary')\n",
    "\n",
    "    \"\"\" Painting the matrix cells \"\"\"\n",
    "    for i in range(len(row_headers)):\n",
    "        for j in range(len(df.columns)):\n",
    "            if np.isnan(matrix[i, j]):\n",
    "                color = 'black'\n",
    "            elif matrix[i, j] == 1:\n",
    "                color = 'red'\n",
    "            elif matrix[i, j] == 0:\n",
    "                color = 'white'\n",
    "            else:\n",
    "                color = 'grey'\n",
    "\n",
    "            rect = plt.Rectangle((j - 0.5, i - 0.5), 1, 1, fill=True, color=color)\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "    ax.set_xticks(np.arange(len(col_headers)))\n",
    "    ax.set_yticks(np.arange(len(row_headers)))\n",
    "    ax.set_xticklabels(list(string.ascii_uppercase)[:len(col_headers)])\n",
    "    ax.set_yticklabels(row_headers)\n",
    "    plt.title(f'DSM {setname}')\n",
    "    plt.yticks(fontsize=8)\n",
    "    plt.xticks(fontsize=8)\n",
    "    plt.savefig(\"support/%s/DSM/DSM_%s.png\" % (search_terms, setname), bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Display DSM \"\"\"\n",
    "setnames = ['setting_1', 'setting_2', 'setting_3', 'setting_4', 'setting_5']\n",
    "\n",
    "for setname in setnames:\n",
    "    print(setname)\n",
    "    DSM_display(f'support/{search_terms}/DSM/{setname}.csv', setname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
